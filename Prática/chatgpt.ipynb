{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dntfreitas/inteligencia-artificial/blob/main/Pr%C3%A1tica/chatgpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "742da49ee4718c8f",
      "metadata": {
        "id": "742da49ee4718c8f"
      },
      "source": [
        "# Importação das bibliotecas necessárias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c3a61b749a543ed",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-23T10:53:05.949706Z",
          "start_time": "2023-11-23T10:52:57.157301Z"
        },
        "id": "7c3a61b749a543ed"
      },
      "outputs": [],
      "source": [
        "# Instalação das bibliotecas necessárias\n",
        "!pip install openai==0.28\n",
        "!pip install python-dotenv\n",
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "950eadc84e5c9223",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-23T10:52:37.428384Z",
          "start_time": "2023-11-23T10:52:37.419496Z"
        },
        "id": "950eadc84e5c9223"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pprint\n",
        "\n",
        "import gradio as gr\n",
        "import openai\n",
        "from IPython.display import display, Markdown\n",
        "from dotenv import load_dotenv, find_dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a963df423457b37",
      "metadata": {
        "id": "5a963df423457b37"
      },
      "source": [
        "# Configuração da API por variáveis de ambiente\n",
        "\n",
        "A chave da API pode ser obtida em [https://platform.openai.com/api-keys](https://platform.openai.com/api-keys).\n",
        "\n",
        "A chave deverá depois ser guardada num ficheiro chamado, por exemplo, `.env` com o seguinte conteúdo:\n",
        "\n",
        "```\n",
        "OPENAI_API_KEY=chave-da-api\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e79e0b6c641b793b",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-23T10:27:57.039988Z",
          "start_time": "2023-11-23T10:27:57.033262Z"
        },
        "id": "e79e0b6c641b793b"
      },
      "outputs": [],
      "source": [
        "_ = load_dotenv(find_dotenv())\n",
        "\n",
        "openai.api_key = os.getenv('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ccca94da5ce3b58",
      "metadata": {
        "id": "5ccca94da5ce3b58"
      },
      "source": [
        "# Funções auxiliares\n",
        "\n",
        "O modelo a ser utilizado é o `gpt-3.5-turbo`. Contudo, podem ser utilizados outros modelos. A lista de modelos disponíveis pode ser obtida através do comando `openai.Engine.list()`.\n",
        "\n",
        "A temperatura é um parâmetro que controla a aleatoriedade das respostas. Quanto maior for a temperatura, mais aleatórias serão as respostas. O valor por omissão é 0, pelo que as respostas serão sempre as mesmas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b205b16ba6ccb30",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-23T10:56:38.022768Z",
          "start_time": "2023-11-23T10:56:38.021017Z"
        },
        "id": "3b205b16ba6ccb30"
      },
      "outputs": [],
      "source": [
        "def get_completion(prompt, model=\"gpt-3.5-turbo\", history=None, temperature=0):\n",
        "    if history is None or len(history) == 0:\n",
        "        history = [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are an assitant...\"  # persona que o bot assume\n",
        "            }\n",
        "        ]\n",
        "    history.append({\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    })\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=history,\n",
        "        temperature=temperature,\n",
        "\n",
        "    )\n",
        "    completion = response.choices[0].message[\"content\"]\n",
        "    history.append({\n",
        "        \"role\": \"system\",\n",
        "        \"content\": completion\n",
        "    })\n",
        "    return completion, history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acd257591465eba5",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-23T10:56:38.137614Z",
          "start_time": "2023-11-23T10:56:38.133117Z"
        },
        "id": "acd257591465eba5"
      },
      "outputs": [],
      "source": [
        "def print_as_md(completion):\n",
        "    display(Markdown(completion))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95d578ebe26b0430",
      "metadata": {
        "id": "95d578ebe26b0430"
      },
      "source": [
        "# Exemplo de utilização"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "debfa3195d7d207d",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-23T10:56:44.876625Z",
          "start_time": "2023-11-23T10:56:38.549333Z"
        },
        "id": "debfa3195d7d207d"
      },
      "outputs": [],
      "source": [
        "completion, _ = get_completion(\"What is a neural network?\")\n",
        "print_as_md(completion)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99de33853c21a734",
      "metadata": {
        "id": "99de33853c21a734"
      },
      "source": [
        "**Tente, agora, alterar o prompt para obter respostas diferentes, alterar a temperatura para perceber como esta afeta as respostas e alterar o modelo para perceber como este afeta as respostas.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4efc3ea945fe0491",
      "metadata": {
        "id": "4efc3ea945fe0491"
      },
      "source": [
        "## Criação de um chatbot"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd5c4aba8ca6a5a",
      "metadata": {
        "id": "dd5c4aba8ca6a5a"
      },
      "source": [
        "Para a criação de um chabot, precisamos de guardar o contexto, isto é, o histórico de mensagens entre o utilizador e o chatbot. Desta forma, o chatbot pode dar respostas mais adequadas ao contexto da conversa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22ab162d59320ab4",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-23T10:57:47.457049Z",
          "start_time": "2023-11-23T10:57:47.446329Z"
        },
        "id": "22ab162d59320ab4"
      },
      "outputs": [],
      "source": [
        "history = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a98aabfb54356a37",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-23T10:57:54.086749Z",
          "start_time": "2023-11-23T10:57:50.483412Z"
        },
        "id": "a98aabfb54356a37"
      },
      "outputs": [],
      "source": [
        "completion, history = get_completion(\"What is a neural network?\", history=history)\n",
        "print_as_md(completion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71ce1d9fe79fc2ca",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-23T10:58:04.223163Z",
          "start_time": "2023-11-23T10:57:57.681888Z"
        },
        "id": "71ce1d9fe79fc2ca"
      },
      "outputs": [],
      "source": [
        "completion, history = get_completion(\"Hum... explain me as if I was a 5 year old.\", history=history)\n",
        "print_as_md(completion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "901d6a2c1f1b12cb",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-23T10:58:04.224095Z",
          "start_time": "2023-11-23T10:58:04.217259Z"
        },
        "id": "901d6a2c1f1b12cb"
      },
      "outputs": [],
      "source": [
        "pprint.pprint(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "791b5e7bcf772773",
      "metadata": {
        "id": "791b5e7bcf772773"
      },
      "source": [
        "### Vamos, agora, criar uma app para interagir com o chatbot"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afa9a8db71137358",
      "metadata": {
        "id": "afa9a8db71137358"
      },
      "source": [
        "Para tal, vamos usar a biblioteca [Gradio](https://www.gradio.app/), que permite criar interfaces para modelos de Machine Learning, neste caso, para o nosso chatbot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3b39cd202a2c3e7",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-23T11:02:30.971648Z",
          "start_time": "2023-11-23T11:02:30.968854Z"
        },
        "id": "b3b39cd202a2c3e7"
      },
      "outputs": [],
      "source": [
        "def chat_wrapper(prompt, history):\n",
        "    history_arg = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are an assitant...\"  # persona que o bot assume\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Since the history is a list of lists, such as [[user_message, bot_message], ...], we need to convert it to a list of dictionaries, such as [{\"role\": \"user\", \"content\": user_message}, {\"role\": \"system\", \"content\": bot_message}, ...]\n",
        "    for i in range(len(history)):\n",
        "        user_message = history[i][0]\n",
        "        bot_message = history[i][1]\n",
        "\n",
        "        history_arg.append({\n",
        "            \"role\": \"user\",\n",
        "            \"content\": user_message\n",
        "        })\n",
        "\n",
        "        history_arg.append({\n",
        "            \"role\": \"system\",\n",
        "            \"content\": bot_message\n",
        "        })\n",
        "\n",
        "    history_arg.append({\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    })\n",
        "\n",
        "    completion, _ = get_completion(prompt, history=history_arg)\n",
        "\n",
        "    return completion\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcfe11ed8ff425d7",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-23T11:02:45.281337Z",
          "start_time": "2023-11-23T11:02:45.258245Z"
        },
        "id": "dcfe11ed8ff425d7"
      },
      "outputs": [],
      "source": [
        "chat = gr.ChatInterface(\n",
        "    chat_wrapper\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7960641c3776796",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-23T11:02:48.278713Z",
          "start_time": "2023-11-23T11:02:48.251232Z"
        },
        "id": "c7960641c3776796"
      },
      "outputs": [],
      "source": [
        "chat.launch(share=True)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}